# Lexer

## What is lexing

Lexing is the process of separating a string of characters into different sections. The lexing library that **Fin** uses operates in two levels, first it tries to identify sentences and convert this string of text into an array of sentences, then on each sentence, it tries to identify tokens (words, punctuations ...etc) and convert a sentence into an array of token.

[![lexer steps](../images/lexer.png)](../images/lexer.png)

Although this might seem like a trivial task that can be done by `"string".split(" ")`, it's not, it has it's own challenges. For example, a URL can be encapsulated inside parenthesis:

```bash
Alex sent me a link (www.wikipedia.com) to wikipedia
```

But some URLs have parenthesis of their own, like this one

```bash
Alex sent me a link (https://en.wikipedia.org/wiki/Recursion_(computer_science)) describing recursion in programming.
```

And that barely touches the problem.

> __HINT__
> The lexer library is `~99%` compliant with the penn treebank corpus. Which makes it the best natural natural language processing lexer ever implemented in javascript.

## Example

```javascript
const fin = require("finnlp");

var input = "O'Relly Media (formerly O'Relly Associates), is a 49%-owned company. I didn't address any of the emails. Mr. T.J., an employee is at E!. He said: \"$4,000 was the profit.\" We met T.J. around 08:30 in the morning.";
// This is quite complex paragraph of text.
// We should learn a lot by running it through the lexer.

var processed = new fin(input);

```

Let's see the lexing processing result:

* `console.log(processed.sentences)`: logs an array of sentences
```javascript
[
	"O'Relly Media (formerly O'Relly Associates), is a 49%-owned company.",
	"I didn't address any of the emails.",
	"Mr. T.J., an employee is at E!.",
	"He said: \"$4,000 was the profit.\"",
	"We met T.J. around 08:30 in the morning."
]
```

* `console.log(processed.result)`: logs an array of the result of processing each sentence
```javascript
[
	{
		// an object that has the tokens,
		// the POS tags
		// and the dependency parsing results.
	},
	{/* ... */},
	{/* ... */},
	{/* ... */}
]
```

Since we're not interested in all the results for now, let's only log the tokens:

* `console.log(processed.result[0].tokens)`
```javascript
[
	"O'Relly",
	"Media",
	"(",
	"formerly",
	"O'Relly",
	"Associates",
	")",
	",",
	"is",
	"a",
	"49%-owned",
	"company",
	"."
]
```

* `console.log(processed.result[1].tokens)`
```javascript
["I","did","n't","address","any","of","the","emails","."]
```

* `console.log(processed.result[2].tokens)`
```javascript
["Mr.","T.J.",",","an","employee","is","at","E!","."]
```

* `console.log(processed.result[3].tokens)`
```javascript
["He","said",":","\"","$4,000","was","the","profit",".","\""]
```

* `console.log(processed.result[4].tokens)`
```javascript
["We","met","T.J.","around","08:30","in","the","morning","."]
```


## Lexing Details

### Separating sentences

The lexer identifies a sentence end by looking for punctuation marks that are usually found at the end of the sentence.

- **Full Stop**: `.`
- **Exclamation Mark**: `!`
- **Question Mark**: `?`
- **Ellipses**: `â€¦` or `...`

The above punctuation marks are considered separators between tokens.

> __NOTE__
> The lexer actually does more than that, by seeing for example if the full stop punctuation mark came after an abbreviation like: `Morty Jr. had fun last night with Mr. Barney`. and it also detects those sentences where the full stop mark is included inside the parenthesis or the quotation like: `I felt I'm "losing my mind." It was obvious.`

### Separating tokens

- Every two words that are separated by space are considered two different tokens.
- Every word that has a non-word character is considered to be consisting of multiple tokens, except for those cases:
	- Ratios and times: `1:4` `09:30`
	- Compound words: `Geo-location` `T.F.-based` `49%-owned`
	- Decade ranges: `1970's` `1970s`
	- Special punctuations that should be considered as one token: `...` `--`
	- Special cases with numbers: `45$` `45USD` `25nd` `'25`
	- English language contractions: `'d` `n't` `'s`
	- Common abbreviations: `Mr.` `Mrs.` `Jr.`
	- Acronyms: `U.S.` `U.K.`
	- Special cases with proper nouns: `E!` `O'Donnel` `4SQ'EM`

So based on the above rules you'll get an array of tokens.


## Extending the lexer

To extend the lexer:

```javascript
const fin = require("finnlp");
fin.extend({
	id:"lexer-transformer",
	extension:{
		// the extension object should be compliant with
		// lexed library extension API
		// https://github.com/FinNLP/lexed#extensibility
	}
})
```


## Standalone usage

The lexer library can also be used as a standalone package:

```bash
npm i --save lexed
```

For more about the lexer library (i.e. **lexed**), refer to it's [readme.md](https://github.com/FinNLP/lexed/blob/master/readme.md).
